#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import subprocess
import sys
import shutil
import glob
import re
import json
from pathlib import Path
import urllib.request
import urllib.error
import urllib.parse
import base64
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse

# ================== C·∫§U TR√öC TH∆Ø M·ª§C ==================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # download/main/python/
MAIN_DIR = os.path.dirname(SCRIPT_DIR)  # download/main/
DOWNLOAD_DIR = os.path.dirname(MAIN_DIR)  # download/

ENGINE_DIR = os.path.join(MAIN_DIR, "engine")
YTDLP_EXE = os.path.join(ENGINE_DIR, "yt-dlp.exe")
FFMPEG_EXE = os.path.join(ENGINE_DIR, "ffmpeg.exe")

COOKIES_FILE = os.path.join(MAIN_DIR, "cookies.txt")
SOURCE_FILE = os.path.join(MAIN_DIR, "source.txt")

GETLINK_DIR = os.path.join(MAIN_DIR, "getlink")
LINK_FILE = os.path.join(GETLINK_DIR, "link.txt")

FINISH_DIR = os.path.join(DOWNLOAD_DIR, "finish")

# Mode 3 specific folders
MODE3_DIR = os.path.join(FINISH_DIR, "mode3")
MODE3_OTHER_DIR = os.path.join(MODE3_DIR, "other")

# Global flag to indicate mode3 active
MODE3_ACTIVE = False

# Threadpool config for mode3 downloads
MAX_WORKERS = 6  
# you can increase/decrease depending on your environment

# Threadpool config for mode1 (Facebook)
MODE1_MAX_WORKERS = 6  # s·ªë lu·ªìng ƒë·ªìng th·ªùi cho ch·∫ø ƒë·ªô 1

# Server process management
SERVER_FILE = os.path.join(SCRIPT_DIR, "server.py")
server_process = None  # Global variable to track server process

# ================== REGEX PATTERN ==================
VIDEO_URL_REGEX = re.compile(
    r"https://www\.facebook\.com/[a-zA-Z0-9.\-_]+/videos/\d+/"
)

# ================== SERVER MANAGEMENT (ƒê√É CH·ªàNH S·ª¨A) ==================
def is_server_running():
    """Ki·ªÉm tra xem server c√≥ ƒëang ch·∫°y kh√¥ng"""
    global server_process
    if server_process is None:
        return False
    # Check if process is still running
    if server_process.poll() is None:
        return True
    else:
        server_process = None
        return False

def start_server():
    """Kh·ªüi ƒë·ªông server.py"""
    global server_process
    
    if not os.path.isfile(SERVER_FILE):
        print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y file server.py t·∫°i: {SERVER_FILE}")
        print("üìã Vui l√≤ng t·∫°o file server.py trong th∆∞ m·ª•c: " + MAIN_DIR)
        input("\nNh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return False
    
    try:
        # CH·ªàNH S·ª¨A: Th√¥ng b√°o ng·∫Øn g·ªçn
        print("\nüöÄ ƒêang kh·ªüi ƒë·ªông server...")
        
        # Start server process and redirect output to current console
        if sys.platform == "win32":
            # Windows: Use creationflags to show console output
            server_process = subprocess.Popen(
                [sys.executable, SERVER_FILE],
                cwd=MAIN_DIR,
                stdout=sys.stdout,
                stderr=sys.stderr,
                stdin=sys.stdin,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP
            )
        else:
            # Unix-like systems
            server_process = subprocess.Popen(
                [sys.executable, SERVER_FILE],
                cwd=MAIN_DIR,
                stdout=sys.stdout,
                stderr=sys.stderr,
                stdin=sys.stdin,
                preexec_fn=os.setsid
            )
        
        # Wait a bit to check if server started successfully
        time.sleep(1)
        if server_process.poll() is None:
            # CH·ªàNH S·ª¨A: Ch·ªâ hi·ªán th√¥ng b√°o th√†nh c√¥ng v√† return ngay
            print("‚úÖ Server ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!")
            return True
        else:
            print("‚ùå Server kh·ªüi ƒë·ªông th·∫•t b·∫°i (process terminated immediately)")
            server_process = None
            input("\nNh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
            return False
            
    except Exception as e:
        print(f"\n‚ùå L·ªói khi kh·ªüi ƒë·ªông server: {str(e)}")
        server_process = None
        input("\nNh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return False

def stop_server():
    """T·∫Øt server ƒëang ch·∫°y"""
    global server_process
    
    if server_process is None:
        print("\n‚ö†Ô∏è  Kh√¥ng c√≥ server n√†o ƒëang ch·∫°y")
        # CH·ªàNH S·ª¨A: B·ªè input wait
        return False
    
    try:
        print("\nüõë ƒêang t·∫Øt server...")
        
        if sys.platform == "win32":
            # Windows: Terminate process
            server_process.terminate()
            try:
                server_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                server_process.kill()
                server_process.wait()
        else:
            # Unix-like: Kill process group
            try:
                os.killpg(os.getpgid(server_process.pid), 15)  # SIGTERM
                server_process.wait(timeout=5)
            except:
                try:
                    os.killpg(os.getpgid(server_process.pid), 9)  # SIGKILL
                except:
                    pass
        
        # CH·ªàNH S·ª¨A: Th√¥ng b√°o xong v√† return ngay
        print("‚úÖ Server ƒë√£ ƒë∆∞·ª£c t·∫Øt")
        server_process = None
        return True
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi t·∫Øt server: {str(e)}")
        # Force reset
        server_process = None
        input("\nNh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return False

def toggle_server():
    """Toggle server state (start if stopped, stop if running)"""
    if is_server_running():
        return stop_server()
    else:
        return start_server()

# ================== KI·ªÇM TRA FILE ENGINE ==================
def check_engine_files():
    """Ki·ªÉm tra yt-dlp.exe v√† ffmpeg.exe"""
    missing = []

    if not os.path.isfile(YTDLP_EXE):
        missing.append(("yt-dlp.exe", ENGINE_DIR))

    if not os.path.isfile(FFMPEG_EXE):
        missing.append(("ffmpeg.exe", ENGINE_DIR))

    if missing:
        error_msg = f"‚ùå THI·∫æU {len(missing)} FILE ENGINE:\n"
        for filename, directory in missing:
            error_msg += f"   - {filename} (c·∫ßn ƒë·∫∑t trong: {directory})\n"
        error_msg += "\nüìã H∆Ø·ªöNG D·∫™N:\n   1. T·∫£i c√°c file c√≤n thi·∫øu\n   2. ƒê·∫∑t v√†o ƒë√∫ng th∆∞ m·ª•c nh∆∞ tr√™n\n   3. Ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh"
        return False, error_msg

    return True, None


# ================== MOVE FINISHED FILE (D√ô PH·∫¶N L·ªöN KH√îNG C·∫¶N CHO MODE1) ==================
def move_finished_videos(before_files):
    """Di chuy·ªÉn video ƒë√£ t·∫£i v√†o th∆∞ m·ª•c finish/ (phi√™n b·∫£n c≈© cho c√°c ch·∫ø ƒë·ªô kh√°c)"""
    after_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))
    new_files = after_files - before_files

    for file in new_files:
        base = os.path.basename(file)
        dst = os.path.join(FINISH_DIR, base)
        try:
            shutil.move(file, dst)
            print(f"üìÅ ƒê√£ chuy·ªÉn v√†o finish/: {base}")
        except Exception:
            try:
                shutil.move(file, dst)
            except:
                pass


def move_new_files_to_mode3(before_files, output_dir=DOWNLOAD_DIR):
    """
    Di chuy·ªÉn c√°c t·ªáp m·ªõi (sau download) v√†o finish/mode3 (mp3/mp4)
    C√°c file kh√°c -> finish/mode3/other
    Tr·∫£ v·ªÅ danh s√°ch c√°c file ƒë√£ chuy·ªÉn (tuples: (src, dst))
    """
    os.makedirs(MODE3_DIR, exist_ok=True)
    os.makedirs(MODE3_OTHER_DIR, exist_ok=True)

    after_files = set(glob.glob(os.path.join(output_dir, "*.*")))
    new_files = after_files - before_files
    moved = []

    for file in new_files:
        name = os.path.basename(file)
        ext = os.path.splitext(name)[1].lower()
        try:
            if ext in (".mp4", ".mp3"):
                dst = os.path.join(MODE3_DIR, name)
            else:
                dst = os.path.join(MODE3_OTHER_DIR, name)

            # If destination exists, add timestamp suffix
            if os.path.exists(dst):
                base, e = os.path.splitext(name)
                dst = os.path.join(os.path.dirname(dst), f"{base}_{int(time.time())}{e}")

            shutil.move(file, dst)
            moved.append((file, dst))
        except Exception as e:
            # best effort
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ di chuy·ªÉn {name}: {str(e)[:80]}")
    return moved


# ========================================================================
# ======================= CH·∫æ ƒê·ªò 1: FACEBOOK =============================
# ========================================================================

def extract_links_silent():
    """ƒê·ªçc source.txt ‚Üí xu·∫•t link.txt v√†o folder getlink/"""
    os.makedirs(GETLINK_DIR, exist_ok=True)

    source_path = Path(SOURCE_FILE)

    if not source_path.exists():
        return False, f"‚ùå THI·∫æU FILE: {SOURCE_FILE}\n\nüìã H∆Ø·ªöNG D·∫™N:\n   1. T·∫°o file 'source.txt' trong th∆∞ m·ª•c: {MAIN_DIR}\n   2. D√°n HTML source code c·ªßa Facebook v√†o file ƒë√≥\n   3. Ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh"

    try:
        raw_html = source_path.read_text(encoding="utf-8", errors="ignore")
        html = raw_html.replace("\\/", "/")
        matches = set(VIDEO_URL_REGEX.findall(html))

        output_path = Path(LINK_FILE)
        if output_path.exists():
            output_path.unlink()

        with open(LINK_FILE, "w", encoding="utf-8") as f:
            for link in sorted(matches):
                f.write(link + "\n")

        return True, None

    except Exception as e:
        return False, f"‚ùå L·ªñI khi x·ª≠ l√Ω source.txt: {str(e)}"


def check_facebook_files():
    """Ki·ªÉm tra cookies.txt cho Facebook"""
    if not os.path.isfile(COOKIES_FILE):
        return False, f"‚ùå THI·∫æU FILE: cookies.txt\n\nüìã H∆Ø·ªöNG D·∫™N:\n   1. T·∫°o file 'cookies.txt' trong th∆∞ m·ª•c: {MAIN_DIR}\n   2. Export cookies t·ª´ Facebook\n   3. Ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh"
    return True, None


def is_cookie_expired(stderr_text: str) -> bool:
    """Ki·ªÉm tra cookies c√≥ h·∫øt h·∫°n kh√¥ng"""
    keywords = [
        "login required",
        "you must log in",
        "this video is private",
        "http error 403",
        "forbidden",
        "cookies",
        "unsupported url"
    ]

    text = (stderr_text or "").lower()
    return any(k in text for k in keywords)


def download_facebook_video(url):
    """T·∫£i video t·ª´ URL Facebook (phi√™n b·∫£n c≈©, ƒë·ªìng b·ªô)"""
    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--cookies", COOKIES_FILE,
        "--no-playlist",
        "-f", "bv*+ba/b",
        "--merge-output-format", "mp4",
        url
    ]

    print("\n‚¨á ƒêang t·∫£i...\n")

    result = subprocess.run(
        cmd,
        stdout=None,
        stderr=subprocess.PIPE,
        text=True
    )

    if result.returncode != 0:
        if is_cookie_expired(result.stderr):
            print("\nüö® COOKIES FACEBOOK ƒê√É H·∫æT H·∫†N / KH√îNG H·ª¢P L·ªÜ")
            print("üëâ Vui l√≤ng export l·∫°i cookies.txt r·ªìi ch·∫°y l·∫°i tool.")
            input("\nNh·∫•n Enter ƒë·ªÉ tho√°t...")
            sys.exit(1)

        print("‚ùå T·∫£i th·∫•t b·∫°i (l·ªói kh√°c).")
        return

    print("‚úÖ T·∫£i xong.")
    move_finished_videos(before_files)


def facebook_mode_manual():
    """Facebook: Nh·∫≠p link th·ªß c√¥ng"""
    while True:
        url = input("\nD√°n URL video Facebook (q ƒë·ªÉ quay l·∫°i):\n> ").strip()

        if url.lower() in ("q", "quit", "exit"):
            break

        if not url:
            print("‚ö†Ô∏è  URL tr·ªëng.")
            continue

        download_facebook_video(url)


# ========== New: multi-threaded Facebook worker and parallel runner (WRITE DIRECTLY TO FINISH) ==========

def facebook_download_worker(tmp_index, url):
    """
    Worker ch·∫°y yt-dlp cho m·ªôt URL Facebook **v√† ghi tr·ª±c ti·∫øp v√†o FINISH_DIR**.
    S·ª≠ d·ª•ng unique_suffix (timestamp + index) ƒë·ªÉ tr√°nh xung ƒë·ªôt t√™n.
    Tr·∫£ v·ªÅ dict: {"url": url, "success": bool, "error": str or None, "cookie_expired": bool, "files": [paths]}
    """
    result_info = {"url": url, "success": False, "error": None, "cookie_expired": False, "files": []}
    unique_suffix = f"{int(time.time())}_{tmp_index}"
    output_template = os.path.join(FINISH_DIR, f"%(title)s_{unique_suffix}.%(ext)s")

    # Ensure finish dir exists
    os.makedirs(FINISH_DIR, exist_ok=True)

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--cookies", COOKIES_FILE,
        "--no-playlist",
        "-f", "bv*+ba/b",
        "--merge-output-format", "mp4",
        "-o", output_template,
        url
    ]

    try:
        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=3600)
        stderr = proc.stderr or ""
        stdout = proc.stdout or ""

        if proc.returncode != 0:
            # check cookie expired
            if is_cookie_expired(stderr):
                result_info["error"] = "cookies_expired"
                result_info["cookie_expired"] = True
                return result_info

            # otherwise failure
            # Capture some stderr for debugging but keep concise
            err = stderr.strip().splitlines()
            result_info["error"] = err[-1][:400] if err else "unknown_error"
            return result_info

        # Success: locate files in FINISH_DIR that include our unique_suffix
        pattern = os.path.join(FINISH_DIR, f"*_{unique_suffix}.*")
        matched = glob.glob(pattern)
        # It's possible yt-dlp used different naming (rare). If none matched, attempt to find recently modified files.
        if not matched:
            # fallback: find files modified in last 120 seconds
            now = time.time()
            recent = []
            for f in glob.glob(os.path.join(FINISH_DIR, "*.*")):
                try:
                    mtime = os.path.getmtime(f)
                    if now - mtime < 120:
                        recent.append(f)
                except:
                    continue
            matched = recent

        # Record found files
        for f in matched:
            # ignore temporary .Part or incomplete? include them as user requested direct writes
            result_info["files"].append(f)

        if result_info["files"]:
            result_info["success"] = True
        else:
            # If no files found, still treat as failure
            result_info["error"] = result_info.get("error") or "no_files_found_after_ytdlp"
            result_info["success"] = False

        return result_info

    except subprocess.TimeoutExpired:
        result_info["error"] = "timeout"
        return result_info
    except Exception as e:
        result_info["error"] = f"exception: {str(e)}"
        return result_info


def facebook_mode_from_file():
    """Facebook: ƒê·ªçc link t·ª´ file link.txt v√† t·∫£i nhi·ªÅu lu·ªìng c√πng l√∫c (ghi tr·ª±c ti·∫øp v√†o finish/)"""
    if not os.path.isfile(LINK_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y {LINK_FILE}")
        print("üí° H√£y ch·∫°y l·∫°i t·ª´ ƒë·∫ßu ho·∫∑c t·∫°o file link.txt th·ªß c√¥ng")
        return

    with open(LINK_FILE, "r", encoding="utf-8") as f:
        links = [l.strip() for l in f if l.strip()]

    if not links:
        print("‚ö†Ô∏è  link.txt tr·ªëng, kh√¥ng c√≥ link n√†o ƒë·ªÉ t·∫£i.")
        return

    # ki·ªÉm tra cookies tr∆∞·ªõc khi ch·∫°y ƒëa lu·ªìng
    success, error = check_facebook_files()
    if not success:
        print(error)
        input("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c. Nh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return

    total = len(links)
    print(f"\nüìÑ ƒêang t·∫£i {total} link t·ª´ link.txt (ƒëa lu·ªìng v·ªõi {MODE1_MAX_WORKERS} workers). Ghi tr·ª±c ti·∫øp v√†o {FINISH_DIR}\n")

    # Run with ThreadPoolExecutor
    results = []
    cookie_issue_detected = False

    with ThreadPoolExecutor(max_workers=MODE1_MAX_WORKERS) as executor:
        future_to_url = {}
        for i, url in enumerate(links, start=1):
            future = executor.submit(facebook_download_worker, i, url)
            future_to_url[future] = url

        completed = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                info = future.result()
            except Exception as e:
                info = {"url": url, "success": False, "error": f"exception: {str(e)}"}
            results.append(info)
            completed += 1
            status = "OK" if info.get("success") else "FAIL"
            # Minimal progress print
            print(f"Progress: {completed}/{total} ‚Äî {status} ‚Äî {url[:80]}")
            if info.get("cookie_expired"):
                cookie_issue_detected = True

    # Summary
    success_count = sum(1 for r in results if r.get("success"))
    fail_count = total - success_count

    print("\n" + "=" * 60)
    print("üìä T√ìM T·∫ÆT FACEBOOK DOWNLOAD (MODE 1)")
    print("=" * 60)
    print(f"  T·ªïng links: {total}")
    print(f"  T·∫£i th√†nh c√¥ng: {success_count}")
    print(f"  T·∫£i th·∫•t b·∫°i: {fail_count}")
    if cookie_issue_detected:
        print("\nüö® M·ªôt s·ªë t·∫£i th·∫•t b·∫°i do cookies h·∫øt h·∫°n / kh√¥ng h·ª£p l·ªá.")
        print("üëâ Vui l√≤ng c·∫≠p nh·∫≠t file cookies.txt v√† th·ª≠ l·∫°i c√°c links th·∫•t b·∫°i th·ªß c√¥ng.")
    print("=" * 60)


def mode_facebook():
    """Ch·∫ø ƒë·ªô t·∫£i Facebook"""
    print("\n" + "="*60)
    print("üìò CH·∫æ ƒê·ªò: FACEBOOK VIDEO DOWNLOADER")
    print("="*60)

    # Ki·ªÉm tra file
    success, error = extract_links_silent()
    if not success:
        print(error)
        input("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c. Nh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return

    success, error = check_facebook_files()
    if not success:
        print(error)
        input("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c. Nh·∫•n Enter ƒë·ªÉ quay l·∫°i...")
        return

    # Menu Facebook
    while True:
        print("\n1 - Nh·∫≠p link tr·ª±c ti·∫øp")
        print("2 - ƒê·ªçc link t·ª´ link.txt (ƒëa lu·ªìng v√†o finish/)")
        print("q - Quay l·∫°i menu ch√≠nh")

        c = input("> ").strip().lower()

        if c == "1":
            facebook_mode_manual()
        elif c == "2":
            facebook_mode_from_file()
        elif c == "q":
            break
        else:
            print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")


# ========================================================================
# ======================= CH·∫æ ƒê·ªò 2: YOUTUBE =============================
# ========================================================================

def get_youtube_formats(url):
    """Qu√©t c√°c format c·ªßa video YouTube"""
    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--list-formats",
        "--no-playlist",
        url
    ]

    try:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        if result.returncode != 0:
            return None

        return result.stdout

    except Exception as e:
        print(f"‚ùå L·ªói khi qu√©t format: {str(e)}")
        return None


def extract_video_resolutions(format_list):
    """Tr√≠ch xu·∫•t c√°c ƒë·ªô ph√¢n gi·∫£i video t·ª´ danh s√°ch format"""
    resolutions = {}

    lines = format_list.split('\n')
    for line in lines:
        # T√¨m c√°c d√≤ng ch·ª©a th√¥ng tin format
        if 'mp4' in line.lower() and 'x' in line:
            parts = line.split()
            format_id = parts[0] if parts else None

            # T√¨m ƒë·ªô ph√¢n gi·∫£i (v√≠ d·ª•: 1920x1080, 1280x720)
            for part in parts:
                if 'x' in part and part.replace('x', '').replace('p', '').isdigit():
                    resolution = part
                    if resolution not in resolutions and format_id:
                        resolutions[resolution] = format_id
                    break

    return resolutions


def download_youtube_mp3(url):
    """T·∫£i audio t·ª´ YouTube v·ªõi ch·∫•t l∆∞·ª£ng cao nh·∫•t"""
    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--extract-audio",
        "--audio-format", "mp3",
        "--audio-quality", "0",
        "--no-playlist",
        "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
        url
    ]

    print("\n‚¨á ƒêang t·∫£i MP3 (ch·∫•t l∆∞·ª£ng cao nh·∫•t)...\n")

    result = subprocess.run(cmd, stdout=None, stderr=subprocess.PIPE, text=True)

    if result.returncode != 0:
        print("‚ùå T·∫£i th·∫•t b·∫°i.")
        return

    print("‚úÖ T·∫£i xong.")
    move_finished_videos(before_files)


def download_youtube_mp4(url, resolution=None):
    """T·∫£i video t·ª´ YouTube"""
    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    if resolution:
        # T·∫£i v·ªõi ƒë·ªô ph√¢n gi·∫£i c·ª• th·ªÉ
        cmd = [
            YTDLP_EXE,
            "--ffmpeg-location", ENGINE_DIR,
            "-f", f"bestvideo[height<={resolution}]+bestaudio/best[height<={resolution}]",
            "--merge-output-format", "mp4",
            "--no-playlist",
            "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
            url
        ]
    else:
        # T·∫£i v·ªõi ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t
        cmd = [
            YTDLP_EXE,
            "--ffmpeg-location", ENGINE_DIR,
            "-f", "bestvideo+bestaudio/best",
            "--merge-output-format", "mp4",
            "--no-playlist",
            "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
            url
        ]

    print(f"\n‚¨á ƒêang t·∫£i MP4{' (' + str(resolution) + 'p)' if resolution else ' (ch·∫•t l∆∞·ª£ng cao nh·∫•t)'}...\n")

    result = subprocess.run(cmd, stdout=None, stderr=subprocess.PIPE, text=True)

    if result.returncode != 0:
        print("‚ùå T·∫£i th·∫•t b·∫°i.")
        return

    print("‚úÖ T·∫£i xong.")
    move_finished_videos(before_files)


def mode_youtube():
    """Ch·∫ø ƒë·ªô t·∫£i YouTube"""
    print("\n" + "="*60)
    print("üé• CH·∫æ ƒê·ªò: YOUTUBE DOWNLOADER")
    print("="*60)

    while True:
        url = input("\nD√°n URL video YouTube (q ƒë·ªÉ quay l·∫°i):\n> ").strip()

        if url.lower() in ("q", "quit", "exit"):
            break

        if not url:
            print("‚ö†Ô∏è  URL tr·ªëng.")
            continue

        # Ch·ªçn ƒë·ªãnh d·∫°ng
        print("\nCh·ªçn ƒë·ªãnh d·∫°ng:")
        print("1 - MP3 (Audio)")
        print("2 - MP4 (Video)")
        print("q - H·ªßy")

        format_choice = input("> ").strip().lower()

        if format_choice == "q":
            continue
        elif format_choice == "1":
            download_youtube_mp3(url)
        elif format_choice == "2":
            # Qu√©t ƒë·ªô ph√¢n gi·∫£i
            print("\nüîç ƒêang qu√©t c√°c ƒë·ªô ph√¢n gi·∫£i c√≥ s·∫µn...")

            format_list = get_youtube_formats(url)

            if not format_list:
                print("‚ùå Kh√¥ng th·ªÉ qu√©t ƒë·ªô ph√¢n gi·∫£i. T·∫£i v·ªõi ch·∫•t l∆∞·ª£ng m·∫∑c ƒë·ªãnh...")
                download_youtube_mp4(url)
                continue

            resolutions = extract_video_resolutions(format_list)

            if not resolutions:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ƒë·ªô ph√¢n gi·∫£i c·ª• th·ªÉ. T·∫£i v·ªõi ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t...")
                download_youtube_mp4(url)
                continue

            # Hi·ªÉn th·ªã danh s√°ch ƒë·ªô ph√¢n gi·∫£i
            print("\nC√°c ƒë·ªô ph√¢n gi·∫£i c√≥ s·∫µn:")
            sorted_res = sorted(resolutions.keys(), key=lambda x: int(x.split('x')[1]) if 'x' in x else 0, reverse=True)

            for i, res in enumerate(sorted_res, 1):
                print(f"{i} - {res}")
            print("0 - Ch·∫•t l∆∞·ª£ng cao nh·∫•t (auto)")
            print("q - H·ªßy")

            res_choice = input("> ").strip().lower()

            if res_choice == "q":
                continue
            elif res_choice == "0":
                download_youtube_mp4(url)
            elif res_choice.isdigit():
                idx = int(res_choice) - 1
                if 0 <= idx < len(sorted_res):
                    selected_res = sorted_res[idx]
                    # L·∫•y height t·ª´ resolution (v√≠ d·ª•: 1920x1080 ‚Üí 1080)
                    height = selected_res.split('x')[1] if 'x' in selected_res else selected_res.replace('p', '')
                    download_youtube_mp4(url, height)
                else:
                    print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")
            else:
                print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")
        else:
            print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")


# ========================================================================
# ============= CH·∫æ ƒê·ªò 3: ƒêA WEB (N√ÇNG C·∫§P ‚Äî CH·∫æ ƒê·ªò M·ªöI) ================
# ========================================================================

# ---------- C√°c helper fetch/extract (m·ªü r·ªông ƒë·ªÉ t√¨m MP3 n·ªØa) ----------

def fetch_page_source(url):
    """T·∫£i source code c·ªßa trang web"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': url,
        }

        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            content = response.read().decode('utf-8', errors='ignore')
        return content
    except Exception as e:
        # Silent fail for mode3
        return None


def extract_stream_urls(page_source):
    """Tr√≠ch xu·∫•t m3u8/mpd/mp4/mp3 URLs t·ª´ page source (m·ªü r·ªông mp3)"""
    if not page_source:
        return []

    stream_urls = []

    # Pattern cho m3u8
    m3u8_pattern = re.compile(r'(https?://[^\s"\'<>]+\.m3u8[^\s"\'<>]*)', re.IGNORECASE)
    m3u8_matches = m3u8_pattern.findall(page_source)
    stream_urls.extend(m3u8_matches)

    # Pattern cho mpd (DASH)
    mpd_pattern = re.compile(r'(https?://[^\s"\'<>]+\.mpd[^\s"\'<>]*)', re.IGNORECASE)
    mpd_matches = mpd_pattern.findall(page_source)
    stream_urls.extend(mpd_matches)

    # Pattern cho mp4 URLs
    mp4_pattern = re.compile(r'(https?://[^\s"\'<>]+\.mp4[^\s"\'<>]*)', re.IGNORECASE)
    mp4_matches = mp4_pattern.findall(page_source)
    stream_urls.extend(mp4_matches)

    # Pattern cho mp3 URLs (m·ªü r·ªông)
    mp3_pattern = re.compile(r'(https?://[^\s"\'<>]+\.mp3[^\s"\'<>]*)', re.IGNORECASE)
    mp3_matches = mp3_pattern.findall(page_source)
    stream_urls.extend(mp3_matches)

    # Clean URLs (remove HTML entities, quotes, etc.)
    cleaned_urls = []
    for url in stream_urls:
        url = re.sub(r'["\'\)>\]]+$', '', url)
        url = url.replace('&amp;', '&')
        if url and url not in cleaned_urls:
            cleaned_urls.append(url)

    return cleaned_urls


def extract_iframe_sources(page_source):
    """Tr√≠ch xu·∫•t iframe video/audio sources"""
    if not page_source:
        return []

    iframe_pattern = re.compile(r'<iframe[^>]+src=["\']([^"\']+)["\']', re.IGNORECASE)
    iframes = iframe_pattern.findall(page_source)

    video_iframes = []
    media_keywords = ['player', 'embed', 'video', 'stream', 'vimeo', 'youtube', 'jwplayer', 'kaltura', 'brightcove', 'wistia', 'audio', 'soundcloud']

    for iframe in iframes:
        if any(keyword in iframe.lower() for keyword in media_keywords):
            if iframe.startswith('//'):
                iframe = 'https:' + iframe
            elif iframe.startswith('/'):
                # we can attempt to skip or make absolute later
                continue
            video_iframes.append(iframe)

    return video_iframes


def extract_json_video_urls(page_source):
    """Extract video/audio URLs t·ª´ JSON objects trong page source (m·ªü r·ªông mp3)"""
    if not page_source:
        return []

    video_urls = []

    video_keys = [
        'videoUrl', 'videoURL', 'video_url',
        'streamUrl', 'streamURL', 'stream_url',
        'hlsUrl', 'hlsURL', 'hls_url', 'hls',
        'dashUrl', 'dashURL', 'dash_url', 'dash',
        'mp4Url', 'mp4URL', 'mp4_url', 'mp4',
        'mp3Url', 'mp3URL', 'mp3_url', 'mp3',
        'src', 'source', 'url', 'file',
        'playbackUrl', 'playback_url',
        'contentUrl', 'content_url',
        'm3u8', 'mpd'
    ]

    # Try to find JSON-like patterns
    json_patterns = [
        r'\{[^{}]*(?:"(?:' + '|'.join(video_keys) + r')":\s*"([^"]*(?:\.m3u8|\.mpd|\.mp4|\.mp3)[^"]*)")',
        r'\{[^{}]*(?:\'(?:' + '|'.join(video_keys) + r')\'\s*:\s*\'([^\']*(?:\.m3u8|\.mpd|\.mp4|\.mp3)[^\']*)\'))',
    ]

    for pattern in json_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            if isinstance(match, tuple):
                for m in match:
                    if m and ('http' in m or '.m3u8' in m or '.mp3' in m):
                        url = m.replace('\\/', '/')
                        video_urls.append(url)
            elif match and ('http' in match or '.m3u8' in match or '.mp3' in match):
                url = match.replace('\\/', '/')
                video_urls.append(url)

    # JSON-LD schema
    json_ld_pattern = r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
    json_ld_matches = re.findall(json_ld_pattern, page_source, re.DOTALL)

    for json_str in json_ld_matches:
        try:
            data = json.loads(json_str)
            if isinstance(data, dict):
                for key in ['contentUrl', 'embedUrl', 'url']:
                    if key in data and isinstance(data[key], str):
                        if any(ext in data[key] for ext in ['.mp4', '.m3u8', '.mp3']):
                            video_urls.append(data[key])
        except:
            pass

    return list(set(video_urls))


def extract_meta_video_urls(page_source):
    """Extract video/audio URLs t·ª´ meta tags v√† schema (m·ªü r·ªông mp3)"""
    if not page_source:
        return []

    video_urls = []

    og_patterns = [
        r'<meta\s+property=["\']og:video:url["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+property=["\']og:video:secure_url["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+property=["\']og:video["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+content=["\'](.*?)["\']\s+property=["\']og:video["\']',
    ]

    twitter_patterns = [
        r'<meta\s+name=["\']twitter:player:stream["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+name=["\']twitter:player["\']\s+content=["\'](.*?)["\']',
    ]

    schema_patterns = [
        r'"@type"\s*:\s*"VideoObject"[^}]*"contentUrl"\s*:\s*"(.*?)"',
        r'"@type"\s*:\s*"VideoObject"[^}]*"embedUrl"\s*:\s*"(.*?)"',
    ]

    all_patterns = og_patterns + twitter_patterns + schema_patterns

    for pattern in all_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            if match and 'http' in match:
                url = match.replace('&amp;', '&').replace('&#x2F;', '/')
                video_urls.append(url)

    # Also look for audio meta tags (e.g., og:audio)
    audio_patterns = [
        r'<meta\s+property=["\']og:audio["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+property=["\']og:audio:secure_url["\']\s+content=["\'](.*?)["\']',
    ]
    for pattern in audio_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            if match and 'http' in match:
                url = match.replace('&amp;', '&')
                video_urls.append(url)

    return list(set(video_urls))


def extract_video_tag_urls(page_source, base_url=None):
    """Extract URLs t·ª´ <video>, <audio> v√† <source> tags (m·ªü r·ªông ƒë·ªÉ include audio)"""
    if not page_source:
        return []

    video_urls = []

    video_src_patterns = [
        r'<video[^>]+src=["\']([^"\']+)["\']',
        r'<video[^>]+data-src=["\']([^"\']+)["\']',
        r'<video[^>]+data-video-src=["\']([^"\']+)["\']',
    ]

    audio_src_patterns = [
        r'<audio[^>]+src=["\']([^"\']+)["\']',
        r'<audio[^>]+data-src=["\']([^"\']+)["\']',
    ]

    source_patterns = [
        r'<source[^>]+src=["\']([^"\']+)["\']',
        r'<source[^>]+data-src=["\']([^"\']+)["\']',
    ]

    all_patterns = video_src_patterns + audio_src_patterns + source_patterns

    for pattern in all_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            if match:
                if any(ext in match.lower() for ext in ['.mp4', '.webm', '.ogg', '.m3u8', '.mpd', '.mp3', 'audio', 'video']):
                    url = match
                    if url.startswith('//'):
                        url = 'https:' + url
                    elif url.startswith('/'):
                        if base_url:
                            url = urljoin(base_url, url)
                        else:
                            continue
                    video_urls.append(url)

    return list(set(video_urls))


def extract_cdn_video_urls(page_source):
    """Extract video/audio URLs t·ª´ CDN v√† streaming services (m·ªü r·ªông mp3)"""
    if not page_source:
        return []

    video_urls = []

    cdn_patterns = [
        r'https?://[^\s"\']+\.m3u8[^\s"\']*',
        r'https?://[^\s"\']+\.mpd[^\s"\']*',
        r'https?://[^\s"\']+\.mp4[^\s"\']*',
        r'https?://[^\s"\']+\.mp3[^\s"\']*',
        r'https?://[^/]*cloudflare[^/]*[^\s"\']*(?:\.mp4|\.m3u8|\.mp3)?',
        r'https?://[^/]*\.cloudflarestream\.com[^\s"\']*',
        r'https?://[^/]*\.cloudfront\.net[^\s"\']*(?:\.mp4|\.m3u8|\.mp3)?',
        r'https?://[^/]*\.b-cdn\.net[^\s"\']*(?:\.mp4|\.m3u8|\.mp3)?',
        r'https?://[^/]*\.fastly\.net[^\s"\']*(?:\.mp4|\.m3u8|\.mp3)?',
        r'https?://[^/]*\.akamaized\.net[^\s"\']*(?:\.mp4|\.m3u8|\.mp3)?',
        r'https?://[^/]*vimeocdn\.com[^\s"\']*',
        r'https?://[^/]*\.jwplatform\.com[^\s"\']*',
        r'https?://[^/]*\.wistia\.com[^\s"\']*',
        r'https?://[^/]*\.brightcove[^/]*[^\s"\']*',
        r'https?://[^/]*cdn[^/]*\.[^/]+[^\s"\'<>]*(?:\.mp4|\.m3u8|\.mpd|\.mp3)?',
    ]

    for pattern in cdn_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            url = match.replace('\\/', '/').strip().strip('"').strip("'")
            if url and url.startswith('http'):
                video_urls.append(url)

    return list(set(video_urls))


def decode_obfuscated_urls(page_source):
    """Decode c√°c URLs b·ªã encode/obfuscate (m·ªü r·ªông mp3)"""
    if not page_source:
        return []

    decoded_urls = []

    # Base64
    base64_pattern = r'[A-Za-z0-9+/]{40,}={0,2}'
    base64_matches = re.findall(base64_pattern, page_source)
    for b64_str in base64_matches[:50]:
        try:
            decoded = base64.b64decode(b64_str).decode('utf-8', errors='ignore')
            if 'http' in decoded and any(ext in decoded for ext in ['.mp4', '.m3u8', '.mpd', '.mp3']):
                url_match = re.search(r'https?://[^\s"\'<>]+', decoded)
                if url_match:
                    decoded_urls.append(url_match.group(0))
        except:
            continue

    # percent encoded
    encoded_pattern = r'(?:https?%3A%2F%2F|http%3A%2F%2F)[A-Za-z0-9%._-]+'
    encoded_matches = re.findall(encoded_pattern, page_source)
    for encoded_url in encoded_matches:
        try:
            decoded = urllib.parse.unquote(encoded_url)
            if 'http' in decoded and any(ext in decoded for ext in ['.mp4', '.m3u8', '.mpd', '.mp3']):
                decoded_urls.append(decoded)
        except:
            continue

    # unicode escapes
    unicode_pattern = r'(?:\\u[0-9a-fA-F]{4})+'
    unicode_matches = re.findall(unicode_pattern, page_source)
    for unicode_str in unicode_matches[:20]:
        try:
            decoded = unicode_str.encode().decode('unicode-escape')
            if 'http' in decoded and any(ext in decoded for ext in ['.mp4', '.m3u8', '.mpd', '.mp3']):
                url_match = re.search(r'https?://[^\s"\'<>]+', decoded)
                if url_match:
                    decoded_urls.append(url_match.group(0))
        except:
            continue

    # hex encoded
    hex_pattern = r'\\x[0-9a-fA-F]{2}'
    if re.search(hex_pattern, page_source):
        hex_segments = re.findall(r'(?:\\x[0-9a-fA-F]{2}){10,}', page_source)
        for hex_seg in hex_segments[:10]:
            try:
                decoded = bytes.fromhex(hex_seg.replace('\\x', '')).decode('utf-8', errors='ignore')
                if 'http' in decoded:
                    url_match = re.search(r'https?://[^\s"\'<>]+', decoded)
                    if url_match and any(ext in url_match.group(0) for ext in ['.mp4', '.m3u8', '.mpd', '.mp3']):
                        decoded_urls.append(url_match.group(0))
            except:
                continue

    return list(set(decoded_urls))


def extract_all_m3u8_ts_urls(page_source, base_url=None):
    """
    Extract T·∫§T C·∫¢ m3u8 v√† ts URLs t·ª´ page source
    """
    urls = {
        'm3u8': [],
        'ts': []
    }

    if not page_source:
        return urls

    m3u8_patterns = [
        r'https?://[^\s"\'<>]+\.m3u8(?:\?[^\s"\'<>]*)?',
        r'https?://[^\s"\'<>]+\.m3u(?:\?[^\s"\'<>]*)?',
        r'["\']([^"\']+\.m3u8(?:\?[^"\']*)?)["\']',
    ]

    ts_patterns = [
        r'https?://[^\s"\'<>]+\.ts(?:\?[^\s"\'<>]*)?',
        r'["\']([^"\']+\.ts(?:\?[^"\']*)?)["\']',
    ]

    for pattern in m3u8_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            url = match if match.startswith('http') else match
            if url and url not in urls['m3u8']:
                if base_url and not url.startswith('http'):
                    url = urljoin(base_url, url)
                if url.startswith('http'):
                    urls['m3u8'].append(url)

    for pattern in ts_patterns:
        matches = re.findall(pattern, page_source, re.IGNORECASE)
        for match in matches:
            url = match if match.startswith('http') else match
            if url and url not in urls['ts']:
                if base_url and not url.startswith('http'):
                    url = urljoin(base_url, url)
                if url.startswith('http'):
                    urls['ts'].append(url)

    urls['m3u8'] = list(set(urls['m3u8']))
    urls['ts'] = list(set(urls['ts']))

    return urls


# ---------- Download helpers specific for mode3 ----------

def download_direct_file_streaming(url, dest_path, referer=None, timeout=60):
    """Download a direct file (mp3/mp4/other) via streaming to dest_path with headers."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': referer or ''
    }
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=timeout) as response:
            # Determine filename extension if not provided
            CHUNK = 8192
            with open(dest_path, 'wb') as f:
                while True:
                    chunk = response.read(CHUNK)
                    if not chunk:
                        break
                    f.write(chunk)
        return True
    except Exception as e:
        return False


def run_yt_dlp_for_url(url, output_dir, referer=None, extra_args=None):
    """
    S·ª≠ d·ª•ng yt-dlp ƒë·ªÉ t·∫£i URL. output_dir l√† th∆∞ m·ª•c t·∫°m (DOWNLOAD_DIR).
    Tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng (file s·∫Ω ƒë∆∞·ª£c move b·ªüi caller).
    """
    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "-f", "best",
        "--merge-output-format", "mp4",
        "--no-playlist",
        "-o", os.path.join(output_dir, "%(title)s.%(ext)s"),
    ]

    if referer:
        cmd.extend(["--add-header", f"Referer:{referer}"])

    cmd.extend(["--user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"])

    if extra_args:
        cmd.extend(extra_args)

    cmd.append(url)

    try:
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.returncode == 0
    except Exception:
        return False


def mode3_download_worker(url, referer=None, idx=None, total=None):
    """
    Worker to download a single URL in mode3.
    Strategy:
    - If URL contains .m3u8 -> try yt-dlp (optimized HLS), fallback ffmpeg direct (if configured)
    - If URL endswith .mp4/.mp3 (direct file) -> stream download via urllib
    - Otherwise try yt-dlp
    """
    # Display simple progress
    if idx is not None and total is not None:
        prefix = f"Downloading {idx}/{total}: "
    else:
        prefix = "Downloading: "

    # Decide method
    u = url.lower()
    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    try:
        # m3u8 -> use yt-dlp (it handles HLS nicely)
        if ".m3u8" in u or ".mpd" in u:
            success = run_yt_dlp_for_url(url, DOWNLOAD_DIR, referer=referer, extra_args=[
                "--hls-prefer-native",
                "--external-downloader", "ffmpeg",
                "--external-downloader-args", "ffmpeg:-protocol_whitelist file,http,https,tcp,tls,crypto"
            ])
            if success:
                move_new_files_to_mode3(before_files, output_dir=DOWNLOAD_DIR)
                return True, url

            # fallback: try ffmpeg direct (if binary exists)
            try:
                timestamp = int(time.time())
                tmp_out = os.path.join(DOWNLOAD_DIR, f"video_{timestamp}.mp4")
                cmd = [
                    FFMPEG_EXE,
                    "-protocol_whitelist", "file,http,https,tcp,tls,crypto",
                    "-user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                ]
                if referer:
                    cmd.extend(["-headers", f"Referer: {referer}\r\n"])
                cmd.extend(["-i", url, "-c", "copy", "-bsf:a", "aac_adtstoasc", "-y", tmp_out])
                r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if r.returncode == 0 and os.path.exists(tmp_out):
                    moved = move_new_files_to_mode3(set([tmp_out]), output_dir=DOWNLOAD_DIR)
                    return True, url
            except Exception:
                pass

            return False, url

        # direct mp4 or mp3
        if u.endswith(".mp4") or u.endswith(".mp3"):
            # Stream download directly
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename:
                filename = f"file_{int(time.time())}{'.mp3' if u.endswith('.mp3') else '.mp4'}"
            dest_dir = MODE3_DIR if u.endswith((".mp4", ".mp3")) else MODE3_OTHER_DIR
            os.makedirs(dest_dir, exist_ok=True)
            dest_path = os.path.join(dest_dir, filename)
            # avoid overwrite
            if os.path.exists(dest_path):
                base, ext = os.path.splitext(filename)
                dest_path = os.path.join(dest_dir, f"{base}_{int(time.time())}{ext}")
            ok = download_direct_file_streaming(url, dest_path, referer=referer)
            if ok and os.path.exists(dest_path):
                return True, url
            else:
                return False, url

        # other: try yt-dlp as general fallback (supports many sources)
        success = run_yt_dlp_for_url(url, DOWNLOAD_DIR, referer=referer)
        if success:
            move_new_files_to_mode3(before_files, output_dir=DOWNLOAD_DIR)
            return True, url

        # if we reach here, fail
        return False, url

    except Exception:
        return False, url


# ---------- Strategy extract-only runners for mode3 ----------
# Each runner returns list of discovered URLs (no download).
# Mode3 will then download these lists concurrently.

def strategy_extractor_hls_ts(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    hls = extract_all_m3u8_ts_urls(page_source, base_url=url)
    results = hls.get('m3u8', []) + hls.get('ts', [])
    return results


def strategy_extractor_streams(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_stream_urls(page_source)


def strategy_extractor_iframes(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_iframe_sources(page_source)


def strategy_extractor_aggressive(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    all_urls = re.findall(r'https?://[^\s"\'<>]+', page_source)
    video_keywords = ['.mp4', '.m3u8', '.mpd', '.mp3', 'video', 'stream', 'media', 'cdn', 'player', 'audio']
    potential_urls = [u for u in all_urls if any(k in u.lower() for k in video_keywords)]
    return list(set(potential_urls))


def strategy_extractor_json(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_json_video_urls(page_source)


def strategy_extractor_meta(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_meta_video_urls(page_source)


def strategy_extractor_video_tags(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_video_tag_urls(page_source, base_url=url)


def strategy_extractor_cdn(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return extract_cdn_video_urls(page_source)


def strategy_extractor_decode(url):
    page_source = fetch_page_source(url)
    if not page_source:
        return []
    return decode_obfuscated_urls(page_source)


# Map of strategy display names -> extractor functions for mode3
MODE3_STRATEGIES = [
    ("HLS/TS SPECIALIST", strategy_extractor_hls_ts),
    ("EXTRACT STREAMS", strategy_extractor_streams),
    ("IFRAME DETECTION", strategy_extractor_iframes),
    ("AGGRESSIVE SCAN", strategy_extractor_aggressive),
    ("WITH COOKIES", strategy_extractor_streams),  # same extractor but mode3 will pass referer/cookies if needed
    ("JSON EXTRACTION", strategy_extractor_json),
    ("META TAGS & SCHEMA", strategy_extractor_meta),
    ("HTML5 VIDEO TAGS", strategy_extractor_video_tags),
    ("CDN DETECTION", strategy_extractor_cdn),
    ("DECODE OBFUSCATED", strategy_extractor_decode),
    # DIRECT YT-DLP not as extractor here; mode3 will attempt fallback via yt-dlp on URLs already found
]


def download_universal_web_mode3(url):
    """
    New Mode 3 workflow:
    1) For each strategy, run its extractor to gather URLs (no downloads)
    2) Only display strategies that found URLs (print number found)
    3) Deduplicate and prepare download queue
    4) Download with ThreadPoolExecutor concurrently
    5) Move files appropriately into finish/mode3 and finish/mode3/other
    6) Print compact progress and a final summary table (only successful strategies shown)
    """
    global MODE3_ACTIVE
    MODE3_ACTIVE = True
    os.makedirs(FINISH_DIR, exist_ok=True)
    os.makedirs(MODE3_DIR, exist_ok=True)
    os.makedirs(MODE3_OTHER_DIR, exist_ok=True)

    print("\n" + "=" * 60)
    print("üåê CH·∫æ ƒê·ªò: ƒêA WEB - MODE 3 (MULTI-THREAD DOWNLOAD)")
    print("=" * 60)
    print(f"\nüéØ Target: {url}\n")
    print("üìå Running multiple extractors and downloading discovered media concurrently...\n")

    strategy_results = {}  # name -> list(urls)
    total_urls_set = set()

    # Run extractors (sequentially; extractors are usually fast). We intentionally do NOT print failures.
    for name, extractor in MODE3_STRATEGIES:
        try:
            urls = extractor(url)
            if urls:
                # normalize urls (unescape basic HTML entities)
                normalized = []
                for u in urls:
                    uu = u.replace('&amp;', '&').replace('\\/', '/').strip()
                    if uu.startswith('//'):
                        uu = 'https:' + uu
                    if uu.startswith('/'):
                        # make absolute if possible
                        parsed_base = urlparse(url)
                        uu = f"{parsed_base.scheme}://{parsed_base.netloc}{uu}"
                    normalized.append(uu)
        
                normalized = list(set(normalized))
                if normalized:
                    strategy_results[name] = normalized
                    for u in normalized:
                        total_urls_set.add(u)
            # If extractor didn't find anything, we do not print anything (hidden)
        except Exception:
            # ignore extractor errors (hidden)
            continue

    if not strategy_results:
        print("‚ùå Kh√¥ng t√¨m th·∫•y media b·∫±ng c√°c strategies mode3.")
        print("‚ÑπÔ∏è G·ª£i √Ω: trang c√≥ th·ªÉ c·∫ßn JavaScript, ƒëƒÉng nh·∫≠p ho·∫∑c c√≥ DRM.")
        return False

    # Print only strategies that succeeded (only counts)
    print("‚úì C√°c strategy t√¨m ƒë∆∞·ª£c URL (ch·ªâ hi·ªÉn th·ªã s·ªë URL):")
    for name, urls in strategy_results.items():
        print(f"  - {name}: {len(urls)} URL(s)")

    all_urls = list(total_urls_set)
    print(f"\nüì• T·ªïng s·ªë URL duy nh·∫•t ƒë∆∞·ª£c t√¨m th·∫•y: {len(all_urls)}")
    if not all_urls:
        print("‚ùå Kh√¥ng c√≥ URL ƒë·ªÉ t·∫£i.")
        return False

    # Prepare download queue: keep order stable but dedup
    all_urls_sorted = sorted(all_urls)
    total = len(all_urls_sorted)

    print("\nüöÄ B·∫Øt ƒë·∫ßu t·∫£i (ƒëa lu·ªìng). Ch·ªâ hi·ªÉn th·ªã ti·∫øn tr√¨nh c∆° b·∫£n.\n")

    # Thread pool executor for concurrent downloads
    results_map = {}  # url -> (success True/False)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {}
        for i, u in enumerate(all_urls_sorted, start=1):
            # Submit worker: we pass idx and total so worker can print progress if desired
            future = executor.submit(mode3_download_worker, u, url, i, total)
            future_to_url[future] = u

        completed = 0
        for future in as_completed(future_to_url):
            u = future_to_url[future]
            try:
                success, _ = future.result(timeout=None)
            except Exception:
                success = False
            results_map[u] = success
            completed += 1
            # Print minimal progress
            print(f"Progress: {completed}/{total} ‚Äî {'OK' if success else 'FAIL'} ‚Äî {u[:80]}")

    # Summarize per-strategy successes (count how many of its urls succeeded)
    strategy_success_counts = {}
    total_success = 0
    for name, urls in strategy_results.items():
        cnt = sum(1 for u in urls if results_map.get(u, False))
        if cnt > 0:
            strategy_success_counts[name] = cnt
            total_success += cnt
        # If cnt == 0, per requirement we hide failures (do not list)

    # Final compact summary table ‚Äî only show successful strategies
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢ T·ªîNG QU√ÅT (ch·ªâ show c√°c strategy th√†nh c√¥ng)")
    print("=" * 60)
    if strategy_success_counts:
        for name, cnt in strategy_success_counts.items():
            print(f"{name:25s}: ‚úÖ {cnt} file(s)")
    else:
        print("‚ùå KH√îNG C√ì STRATEGY N√ÄO T·∫¢I TH√ÄNH C√îNG FILE N√ÄO")

    print("\n" + "=" * 60)
    print(f"üéâ T·ªîNG C·ªòNG ƒê√É T·∫¢I TH√ÄNH C√îNG: {total_success} FILE(S)")
    print("=" * 60)

    return total_success > 0


# ========================================================================
# C√°c strategy c≈© (v·∫´n gi·ªØ cho c√°c ch·∫ø ƒë·ªô kh√°c s·ª≠ d·ª•ng, kh√¥ng thay ƒë·ªïi logic c∆° b·∫£n)
# ========================================================================

def download_from_stream_url(stream_url, referer=None):
    """T·∫£i video t·ª´ stream URL (m3u8/mpd/mp4) ‚Äî original behavior (sequential)"""
    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "-f", "best",
        "--merge-output-format", "mp4",
        "--no-playlist",
        "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
    ]

    if referer:
        cmd.extend(["--add-header", f"Referer:{referer}"])

    cmd.extend(["--user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"])

    cmd.append(stream_url)

    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    if result.returncode == 0:
        move_finished_videos(before_files)
        return True

    return False


def strategy_1_direct_ytdlp(url):
    """STRATEGY 1: T·∫£i tr·ª±c ti·∫øp b·∫±ng yt-dlp (ph∆∞∆°ng ph√°p m·∫∑c ƒë·ªãnh)"""
    print("\nüîß STRATEGY 1: T·∫£i tr·ª±c ti·∫øp v·ªõi yt-dlp...")

    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "-f", "bestvideo+bestaudio/best",
        "--merge-output-format", "mp4",
        "--ignore-errors",
        "--no-playlist",
        "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
        url
    ]

    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    if result.returncode == 0:
        move_finished_videos(before_files)
        print("‚úÖ Strategy 1 th√†nh c√¥ng!")
        return 1

    print("‚ùå Strategy 1 th·∫•t b·∫°i.")
    return 0


def strategy_2_extract_streams(url):
    """STRATEGY 2: Extract m3u8/mpd/mp4 URLs t·ª´ page source"""
    print("\nüîß STRATEGY 2: Ph√°t hi·ªán stream URLs trong page...")
    page_source = fetch_page_source(url)
    if not page_source:
        print("‚ùå Strategy 2 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    stream_urls = extract_stream_urls(page_source)
    if not stream_urls:
        print("‚ùå Strategy 2 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y stream URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(stream_urls)} stream URL(s)")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(stream_urls)} stream URLs...")

    success_count = 0
    for i, stream_url in enumerate(stream_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i stream {i}/{len(stream_urls)}: {stream_url[:80]}...")
        if download_from_stream_url(stream_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng stream {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i stream {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 2: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(stream_urls)} stream(s)")
    else:
        print("\n‚ùå Strategy 2 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c stream n√†o).")

    return success_count


def strategy_3_iframe_detection(url):
    """STRATEGY 3: Detect v√† t·∫£i t·ª´ iframe embeddings"""
    print("\nüîß STRATEGY 3: Ph√°t hi·ªán iframe embeddings...")
    page_source = fetch_page_source(url)
    if not page_source:
        print("‚ùå Strategy 3 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    iframes = extract_iframe_sources(page_source)
    if not iframes:
        print("‚ùå Strategy 3 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y iframe).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(iframes)} iframe(s)")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(iframes)} iframe(s)...")

    success_count = 0
    for i, iframe_url in enumerate(iframes, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i iframe {i}/{len(iframes)}: {iframe_url[:80]}...")
        before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))
        cmd = [
            YTDLP_EXE,
            "--ffmpeg-location", ENGINE_DIR,
            "-f", "best",
            "--merge-output-format", "mp4",
            "--no-playlist",
            "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
            iframe_url
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0:
            move_finished_videos(before_files)
            print(f"‚úÖ T·∫£i th√†nh c√¥ng iframe {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i iframe {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 3: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(iframes)} iframe(s)")
    else:
        print("\n‚ùå Strategy 3 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c iframe n√†o).")

    return success_count


def strategy_4_aggressive_extraction(url):
    """STRATEGY 4: Aggressive extraction - qu√©t t·∫•t c·∫£ URLs c√≥ th·ªÉ"""
    print("\nüîß STRATEGY 4: Aggressive scan - t√¨m m·ªçi URL video...")
    page_source = fetch_page_source(url)
    if not page_source:
        print("‚ùå Strategy 4 th·∫•t b·∫°i.")
        return 0

    all_urls = re.findall(r'https?://[^\s"\'<>]+', page_source)
    video_keywords = ['.mp4', '.m3u8', '.mpd', '.mp3', 'video', 'stream', 'media', 'cdn', 'player']
    potential_urls = [u for u in all_urls if any(k in u.lower() for k in video_keywords)]

    potential_urls = list(set(potential_urls))
    if not potential_urls:
        print("‚ùå Strategy 4 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y potential URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(potential_urls)} potential URL(s)")
    urls_to_try = potential_urls[:20]
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(urls_to_try)} URLs (gi·ªõi h·∫°n 20)...")

    success_count = 0
    for i, video_url in enumerate(urls_to_try, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i URL {i}/{len(urls_to_try)}: {video_url[:80]}...")
        if download_from_stream_url(video_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 4: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(urls_to_try)} URL(s)")
    else:
        print("\n‚ùå Strategy 4 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def strategy_5_with_cookies(url):
    """STRATEGY 5: Th·ª≠ v·ªõi cookies (n·∫øu c√≥ file cookies.txt)"""
    if not os.path.isfile(COOKIES_FILE):
        print("\n‚ö†Ô∏è  STRATEGY 5: B·ªè qua (kh√¥ng c√≥ cookies.txt)")
        return 0

    print("\nüîß STRATEGY 5: Th·ª≠ v·ªõi cookies...")

    before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))

    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--cookies", COOKIES_FILE,
        "-f", "best",
        "--merge-output-format", "mp4",
        "--no-playlist",
        "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
        url
    ]

    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    if result.returncode == 0:
        move_finished_videos(before_files)
        print("‚úÖ Strategy 5 th√†nh c√¥ng!")
        return 1

    print("‚ùå Strategy 5 th·∫•t b·∫°i.")
    return 0


def strategy_6_json_extraction(url):
    """STRATEGY 6: Extract video URLs t·ª´ JSON/API responses"""
    print("\nüîß STRATEGY 6: T√¨m video URLs trong JSON data")
    from urllib.request import Request, urlopen

    try:
        req = Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 6 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    video_urls = extract_json_video_urls(page_source)
    if not video_urls:
        print("‚ùå Strategy 6 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y JSON video URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(video_urls)} video URL(s) trong JSON data")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(video_urls)} URLs...")

    success_count = 0
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i JSON URL {i}/{len(video_urls)}: {video_url[:80]}...")
        if download_from_stream_url(video_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 6: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(video_urls)} URL(s)")
    else:
        print("\n‚ùå Strategy 6 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def extract_meta_video_urls(page_source):
    """(ƒê√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n)"""
    # This function is already implemented above for mode3 extraction.
    return []


def strategy_7_meta_tags(url):
    """STRATEGY 7: Extract t·ª´ meta tags (Open Graph, Twitter Cards, Schema.org)"""
    print("\nüîß STRATEGY 7: T√¨m video qua Meta Tags & Schema...")
    from urllib.request import Request, urlopen

    try:
        req = Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 7 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    video_urls = extract_meta_video_urls(page_source)
    if not video_urls:
        print("‚ùå Strategy 7 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y meta video URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(video_urls)} video URL(s) trong meta tags")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(video_urls)} URLs...")

    success_count = 0
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i meta URL {i}/{len(video_urls)}: {video_url[:80]}...")
        before_files = set(glob.glob(os.path.join(DOWNLOAD_DIR, "*.*")))
        cmd = [
            YTDLP_EXE,
            "--ffmpeg-location", ENGINE_DIR,
            "-f", "best",
            "--merge-output-format", "mp4",
            "--no-playlist",
            "-o", os.path.join(DOWNLOAD_DIR, "%(title)s.%(ext)s"),
            "--referer", url,
            video_url
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0:
            move_finished_videos(before_files)
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 7: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(video_urls)} URL(s)")
    else:
        print("\n‚ùå Strategy 7 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def strategy_8_video_tags(url):
    """STRATEGY 8: Extract t·ª´ HTML5 <video> v√† <source> tags"""
    print("\nüîß STRATEGY 8: Ph√¢n t√≠ch HTML5 video tags...")
    from urllib.request import Request, urlopen

    try:
        req = Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 8 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    video_urls = extract_video_tag_urls(page_source, base_url=url)
    if not video_urls:
        print("‚ùå Strategy 8 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y video tags).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(video_urls)} video URL(s) trong HTML5 tags")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(video_urls)} URLs...")

    success_count = 0
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i video tag {i}/{len(video_urls)}: {video_url[:80]}...")
        if download_from_stream_url(video_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 8: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(video_urls)} URL(s)")
    else:
        print("\n‚ùå Strategy 8 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def strategy_9_cdn_detection(url):
    """STRATEGY 9: Detect video URLs t·ª´ CDN v√† streaming services"""
    print("\nüîß STRATEGY 9: Qu√©t CDN & Streaming Services...")
    from urllib.request import Request, urlopen

    try:
        req = Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 9 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    video_urls = extract_cdn_video_urls(page_source)
    if not video_urls:
        print("‚ùå Strategy 9 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y CDN URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(video_urls)} CDN URL(s)")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(video_urls)} URLs...")

    success_count = 0
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i CDN URL {i}/{len(video_urls)}: {video_url[:80]}...")
        if download_from_stream_url(video_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 9: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(video_urls)} URL(s)")
    else:
        print("\n‚ùå Strategy 9 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def strategy_10_decode_obfuscated(url):
    """STRATEGY 10: Decode encoded/obfuscated URLs"""
    print("\nüîß STRATEGY 10: Gi·∫£i m√£ encoded/obfuscated URLs...")
    from urllib.request import Request, urlopen

    try:
        req = Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 10 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    video_urls = decode_obfuscated_urls(page_source)
    if not video_urls:
        print("‚ùå Strategy 10 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y encoded URLs).")
        return 0

    print(f"‚úì ƒê√£ decode {len(video_urls)} URL(s)")
    print(f"üì• S·∫Ω t·∫£i T·∫§T C·∫¢ {len(video_urls)} URLs...")

    success_count = 0
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n  ‚Üí ƒêang t·∫£i decoded URL {i}/{len(video_urls)}: {video_url[:80]}...")
        if download_from_stream_url(video_url, referer=url):
            print(f"‚úÖ T·∫£i th√†nh c√¥ng URL {i}!")
            success_count += 1
        else:
            print(f"‚ùå T·∫£i th·∫•t b·∫°i URL {i}")

    if success_count > 0:
        print(f"\n‚úÖ Strategy 10: ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{len(video_urls)} URL(s)")
    else:
        print("\n‚ùå Strategy 10 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c URL n√†o).")

    return success_count


def parse_m3u8_playlist(m3u8_url, content=None):
    """(H√†m d√πng trong strategy 11 ‚Äî gi·ªØ nguy√™n, ƒë√£ c√≥ ·ªü file g·ªëc)"""
    from urllib.request import Request, urlopen
    try:
        if content is None:
            req = Request(m3u8_url, headers={'User-Agent': 'Mozilla/5.0'})
            response = urlopen(req, timeout=15)
            content = response.read().decode('utf-8', errors='ignore')
    except:
        return None

    parsed = urlparse(m3u8_url)
    base_url = f"{parsed.scheme}://{parsed.netloc}{'/'.join(parsed.path.split('/')[:-1])}/"

    result = {'master': False, 'variants': [], 'segments': [], 'base_url': base_url}

    lines = content.strip().split('\n')
    if any('#EXT-X-STREAM-INF' in line for line in lines):
        result['master'] = True
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.startswith('#EXT-X-STREAM-INF'):
                resolution = None
                bandwidth = None
                if 'RESOLUTION=' in line:
                    res_match = re.search(r'RESOLUTION=(\d+x\d+)', line)
                    if res_match:
                        resolution = res_match.group(1)
                if 'BANDWIDTH=' in line:
                    bw_match = re.search(r'BANDWIDTH=(\d+)', line)
                    if bw_match:
                        bandwidth = int(bw_match.group(1))
                if i + 1 < len(lines):
                    variant_url = lines[i + 1].strip()
                    if not variant_url.startswith('http'):
                        variant_url = urljoin(m3u8_url, variant_url)
                    result['variants'].append({'url': variant_url, 'resolution': resolution, 'bandwidth': bandwidth})
                i += 2
            else:
                i += 1
    else:
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if line.endswith('.ts') or '.ts?' in line:
                segment_url = line
                if not segment_url.startswith('http'):
                    segment_url = urljoin(m3u8_url, segment_url)
                result['segments'].append(segment_url)

    return result


def find_best_m3u8_variant(variants):
    if not variants:
        return None
    sorted_variants = sorted(variants, key=lambda x: x.get('bandwidth', 0), reverse=True)
    return sorted_variants[0]['url'] if sorted_variants else None


def detect_m3u8_type(url):
    if not ('.m3u8' in url.lower() or url.lower().endswith('.m3u')):
        return None
    try:
        parsed = parse_m3u8_playlist(url)
        if parsed:
            return 'master' if parsed['master'] else 'variant'
    except:
        pass
    return 'unknown'


def download_m3u8_with_ytdlp(m3u8_url, output_dir, referer=None):
    before_files = set(glob.glob(os.path.join(output_dir, "*.*")))
    cmd = [
        YTDLP_EXE,
        "--ffmpeg-location", ENGINE_DIR,
        "--hls-prefer-native",
        "--hls-use-mpegts",
        "--external-downloader", "ffmpeg",
        "--external-downloader-args", "ffmpeg:-protocol_whitelist file,http,https,tcp,tls,crypto",
        "-f", "best",
        "--merge-output-format", "mp4",
        "--no-playlist",
        "--no-warnings",
        "-o", os.path.join(output_dir, "%(title)s.%(ext)s"),
    ]
    if referer:
        cmd.extend(["--add-header", f"Referer:{referer}"])
    cmd.extend(["--user-agent", "Mozilla/5.0"])
    cmd.append(m3u8_url)
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode == 0:
        after_files = set(glob.glob(os.path.join(output_dir, "*.*")))
        new_files = after_files - before_files
        if new_files:
            for file in new_files:
                dst = os.path.join(FINISH_DIR, os.path.basename(file))
                try:
                    shutil.move(file, dst)
                except:
                    pass
            return True
    return False


def download_m3u8_with_ffmpeg_direct(m3u8_url, output_dir, referer=None):
    # fallback method ‚Äî kept small since mode3 primarily uses yt-dlp or ffmpeg direct in worker
    try:
        timestamp = int(time.time())
        output_file = os.path.join(output_dir, f"video_{timestamp}.mp4")
        cmd = [
            FFMPEG_EXE,
            "-protocol_whitelist", "file,http,https,tcp,tls,crypto",
            "-user_agent", "Mozilla/5.0",
        ]
        if referer:
            cmd.extend(["-headers", f"Referer: {referer}\r\n"])
        cmd.extend(["-i", m3u8_url, "-c", "copy", "-bsf:a", "aac_adtstoasc", "-y", output_file])
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0 and os.path.exists(output_file):
            dst = os.path.join(FINISH_DIR, os.path.basename(output_file))
            try:
                shutil.move(output_file, dst)
                return True
            except:
                return False
    except Exception:
        pass
    return False


def download_ts_segments_and_merge(ts_urls, output_dir, referer=None):
    # unchanged from original, kept for completeness
    import tempfile
    if not ts_urls:
        return False

    if len(ts_urls) > 500:
        ts_urls = ts_urls[:500]

    temp_dir = tempfile.mkdtemp(prefix="ts_segments_")
    try:
        downloaded = []
        for i, ts_url in enumerate(ts_urls, 1):
            segment_file = os.path.join(temp_dir, f"segment_{i:04d}.ts")
            try:
                req = urllib.request.Request(ts_url, headers={
                    'User-Agent': 'Mozilla/5.0',
                    'Referer': referer or ''
                })
                response = urllib.request.urlopen(req, timeout=30)
                with open(segment_file, 'wb') as f:
                    f.write(response.read())
                downloaded.append(segment_file)
            except Exception:
                continue

        if not downloaded:
            return False

        concat_file = os.path.join(temp_dir, "concat_list.txt")
        with open(concat_file, 'w') as f:
            for seg_file in downloaded:
                f.write(f"file '{seg_file}'\n")

        timestamp = int(time.time())
        output_file = os.path.join(output_dir, f"merged_video_{timestamp}.mp4")
        cmd = [
            FFMPEG_EXE,
            "-f", "concat",
            "-safe", "0",
            "-i", concat_file,
            "-c", "copy",
            "-bsf:a", "aac_adtstoasc",
            "-y",
            output_file
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0 and os.path.exists(output_file):
            dst = os.path.join(FINISH_DIR, os.path.basename(output_file))
            shutil.move(output_file, dst)
            return True
        else:
            return False
    finally:
        try:
            shutil.rmtree(temp_dir)
        except:
            pass

# ========================================================================
# Strategy 11 (HLS/TS Specialist) ‚Äî original sequential implementation kept
# ========================================================================

def strategy_11_hls_ts_specialist(url):
    print("\nüîß STRATEGY 11: HLS/TS Specialist - Chuy√™n gia M3U8...")
    from urllib.request import Request, urlopen
    try:
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        response = urlopen(req, timeout=15)
        page_source = response.read().decode('utf-8', errors='ignore')
    except:
        print("‚ùå Strategy 11 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c page source).")
        return 0

    hls_urls = extract_all_m3u8_ts_urls(page_source, base_url=url)
    m3u8_urls = hls_urls['m3u8']
    ts_urls = hls_urls['ts']

    if not m3u8_urls and not ts_urls:
        print("‚ùå Strategy 11 th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y m3u8/ts URLs).")
        return 0

    print(f"‚úì T√¨m th·∫•y {len(m3u8_urls)} m3u8 URLs v√† {len(ts_urls)} ts URLs")
    success_count = 0

    if m3u8_urls:
        print(f"\nüì• PHASE 1: X·ª≠ l√Ω {len(m3u8_urls)} M3U8 playlists...")
        for i, m3u8_url in enumerate(m3u8_urls, 1):
            print(f"\n  ‚Üí M3U8 {i}/{len(m3u8_urls)}: {m3u8_url[:80]}...")
            playlist_info = parse_m3u8_playlist(m3u8_url)
            if playlist_info and playlist_info['master']:
                print(f"     üìã Master playlist v·ªõi {len(playlist_info['variants'])} variants")
                best_variant = find_best_m3u8_variant(playlist_info['variants'])
                if best_variant:
                    print(f"     üéØ Best variant: {best_variant[:60]}...")
                    m3u8_url = best_variant

            print("     üîß Method 1: Th·ª≠ v·ªõi yt-dlp...")
            if download_m3u8_with_ytdlp(m3u8_url, DOWNLOAD_DIR, referer=url):
                print(f"     ‚úÖ T·∫£i th√†nh c√¥ng M3U8 {i} b·∫±ng yt-dlp!")
                success_count += 1
                continue

            print("     üîß Method 2: Th·ª≠ v·ªõi ffmpeg direct...")
            if download_m3u8_with_ffmpeg_direct(m3u8_url, DOWNLOAD_DIR, referer=url):
                print(f"     ‚úÖ T·∫£i th√†nh c√¥ng M3U8 {i} b·∫±ng ffmpeg!")
                success_count += 1
                continue

            print(f"     ‚ùå M3U8 {i} th·∫•t b·∫°i c·∫£ 2 methods")

    if ts_urls and success_count == 0:
        print(f"\nüì• PHASE 2: X·ª≠ l√Ω {len(ts_urls)} TS segments.")
        if len(ts_urls) > 50:
            if download_ts_segments_and_merge(ts_urls[:50], DOWNLOAD_DIR, referer=url):
                success_count += 1
        elif len(ts_urls) >= 5:
            if download_ts_segments_and_merge(ts_urls, DOWNLOAD_DIR, referer=url):
                success_count += 1
        else:
            pass

    if success_count > 0:
        print(f"\n‚úÖ Strategy 11: ƒê√£ t·∫£i th√†nh c√¥ng {success_count} file(s)")
    else:
        print("\n‚ùå Strategy 11 th·∫•t b·∫°i (kh√¥ng t·∫£i ƒë∆∞·ª£c file n√†o).")

    return success_count


# ========================================================================
# H√†m download_universal_web_advanced (gi·ªØ cho backward-compatibility)
# ========================================================================

def download_universal_web_advanced(url):
    """T·∫£i video t·ª´ b·∫•t k·ª≥ trang web n√†o - N√ÇNG C·∫§P v·ªõi nhi·ªÅu strategies (phi√™n b·∫£n c≈©)"""
    print("\n" + "=" * 60)
    print("üöÄ B·∫ÆT ƒê·∫¶U MULTI-STRATEGY DOWNLOAD")
    print("=" * 60)
    print(f"\nüéØ Target: {url}\n")
    print("üìå CH·∫†Y T·∫§T C·∫¢ 11 STRATEGIES (kh√¥ng d·ª´ng khi c√≥ 1 c√°i th√†nh c√¥ng)\n")

    strategies = [
        ("HLS/TS SPECIALIST", strategy_11_hls_ts_specialist),
        ("DIRECT YT-DLP", strategy_1_direct_ytdlp),
        ("EXTRACT STREAMS", strategy_2_extract_streams),
        ("IFRAME DETECTION", strategy_3_iframe_detection),
        ("AGGRESSIVE SCAN", strategy_4_aggressive_extraction),
        ("WITH COOKIES", strategy_5_with_cookies),
        ("JSON EXTRACTION", strategy_6_json_extraction),
        ("META TAGS & SCHEMA", strategy_7_meta_tags),
        ("HTML5 VIDEO TAGS", strategy_8_video_tags),
        ("CDN DETECTION", strategy_9_cdn_detection),
        ("DECODE OBFUSCATED", strategy_10_decode_obfuscated),
    ]

    total_downloaded = 0
    results = []

    for strategy_name, strategy_func in strategies:
        try:
            count = strategy_func(url)
            results.append((strategy_name, count))
            total_downloaded += count

            if count > 0:
                print(f"\n‚úÖ {strategy_name}: ƒê√£ t·∫£i {count} file(s)")
            else:
                print(f"\n‚ùå {strategy_name}: Kh√¥ng t·∫£i ƒë∆∞·ª£c file n√†o")

        except Exception as e:
            print(f"‚ùå {strategy_name} g·∫∑p l·ªói: {str(e)}")
            results.append((strategy_name, 0))
            continue

    # T·ªïng k·∫øt
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢ T·ªîNG QU√ÅT")
    print("=" * 60)
    for strategy_name, count in results:
        status = f"‚úÖ {count} file(s)" if count > 0 else "‚ùå Th·∫•t b·∫°i"
        print(f"{strategy_name:20s}: {status}")

    print("\n" + "=" * 60)
    if total_downloaded > 0:
        print(f"üéâ T·ªîNG C·ªòNG ƒê√É T·∫¢I: {total_downloaded} FILE(S)")
        print("=" * 60)
        return True
    else:
        print("‚ùå KH√îNG T·∫¢I ƒê∆Ø·ª¢C FILE N√ÄO")
        print("=" * 60)
        print("\nüí° G·ª£i √Ω:")
        print("   - Video c√≥ th·ªÉ c·∫ßn ƒëƒÉng nh·∫≠p ho·∫∑c c√≥ DRM protection")
        print("   - Th·ª≠ m·ªü trang web trong browser v√† xem c√≥ video kh√¥ng")
        print("   - M·ªôt s·ªë trang y√™u c·∫ßu JavaScript ƒë·ªÉ load video")
        return False


# ========================================================================
# CH·∫æ ƒê·ªò 3 MENU (thay th·∫ø tr∆∞·ªõc ƒë√¢y: mode_universal)
# ========================================================================

def mode_universal():
    """
    CH·∫æ ƒê·ªò 3: ƒêA WEB - Mode3 (m·ªõi)
    L∆∞u √Ω: ƒë√£ lo·∫°i b·ªè ph·∫ßn li·ªát k√™ 'üí™ T√çNH NƒÇNG N√ÇNG CAO' theo y√™u c·∫ßu
    """
    print("\n" + "=" * 60)
    print("üåê CH·∫æ ƒê·ªò: ƒêA WEB - MODE 3 (MULTI-THREAD)")
    print("=" * 60)

    while True:
        url = input("\nD√°n URL trang web ch·ª©a media (q ƒë·ªÉ quay l·∫°i):\n> ").strip()
        if url.lower() in ("q", "quit", "exit"):
            break
        if not url:
            print("‚ö†Ô∏è  URL tr·ªëng.")
            continue

        # Run the new mode3 downloader
        download_universal_web_mode3(url)


# ========================================================================
# ======================= MENU CH√çNH =====================================
# ========================================================================

def main():
    """H√†m ch√≠nh - Menu ch·ªçn ch·∫ø ƒë·ªô"""

    # Ki·ªÉm tra engine files
    success, error = check_engine_files()
    if not success:
        print(error)
        input("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c. Nh·∫•n Enter ƒë·ªÉ tho√°t.")
        sys.exit(1)

    # T·∫°o th∆∞ m·ª•c finish
    os.makedirs(FINISH_DIR, exist_ok=True)

    # Menu ch√≠nh
    while True:
        print("\n" + "=" * 60)
        print("           VIDEO DOWNLOADER - MULTI MODE")
        print("=" * 60)
        print("\nCh·ªçn ch·∫ø ƒë·ªô t·∫£i:")
        print("1 - üìò Facebook Video (c·∫ßn cookies)")
        print("2 - üé• YouTube Video/Audio")
        print("3 - üåê ƒêa Web - Advanced (MODE 3 ƒëa lu·ªìng)")
        
        # Show server status and option
        if is_server_running():
            print("e - üî¥ T·∫Øt Server Local (ƒëang ch·∫°y)")
        else:
            print("e - üü¢ Kh·ªüi ƒë·ªông Server Local")
        
        print("q - Tho√°t")

        choice = input("\n> ").strip().lower()

        if choice == "1":
            mode_facebook()
        elif choice == "2":
            mode_youtube()
        elif choice == "3":
            mode_universal()
        elif choice == "e":
            toggle_server()
        elif choice == "q":
            # If server is running, ask if user wants to stop it before exit
            if is_server_running():
                print("\n‚ö†Ô∏è  Server local ƒëang ch·∫°y!")
                stop_choice = input("B·∫°n c√≥ mu·ªën t·∫Øt server tr∆∞·ªõc khi tho√°t kh√¥ng? (y/n): ").strip().lower()
                if stop_choice == 'y':
                    stop_server()
            print("\nüëã K·∫øt th√∫c.")
            break
        else:
            print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")


if __name__ == "__main__":
    main()